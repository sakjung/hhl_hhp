---
title: "hiphople scraping final"
author: "Jung"
output: html_document
---

```{r setup}
library(httr)
library(XML)
library(RSelenium)
library(rvest)
library(readr)
library(lubridate)
library(tibble)
library(data.table)
library(dplyr)

# Selenium Setting in cmd
# java -Dwebdriver.gecko.driver="geckodriver.exe" -jar selenium-server-standalone-3.141.59.jar -port 4445
```

```{r making functions}
# Some useful functions

# scroll to the webElem
scroll_to <- function(remD, webElem){
  remD$executeScript("arguments[0].scrollIntoView(true);", args = list(webElem))
}

# making scarping break time for politeness
zzz <- function(periods = c(1,1.5)) {
  tictoc <- runif(1, periods[1], periods[2])
  cat(paste0(Sys.time()), "Sleeping for", round(tictoc, 2), "seconds\n")
  Sys.sleep(tictoc)
}

# get the current pagesource
get_html <- function(remD) {
  remD$getPageSource() %>%
    .[[1]] %>%
    read_html()
}
```


```{r open browser}
remD <- remoteDriver(port = 4445L, 
                     browserName = "chrome")
remD$open()
```


```{r crawling pages}
# first page of hiphople
remD$navigate("http://hiphople.com/kboard")

num_pages <- 1
hhl_posts <- data.frame()
l <- list()

for(pageindex in 1:num_pages){
  print(paste(".....","Crawling for posts on page",pageindex))
  page <- get_html(remD)
  
  number <- page %>%
    html_nodes(".no") %>%
    html_text() %>%
    .[-(1:6)] %>%
    trimws(.)
  
  category <- page %>%
    html_nodes(".category") %>%
    html_text() %>%
    .[-(1:10)] %>%
    trimws(.)
  
  # 9 first unnecessary rows & 7 unnecessary last rows
  title <- page %>%
    html_nodes(".title") %>%
    html_text() %>%
    .[-(1:8)] %>%
    head(-7) %>%
    trimws(.)
  
  # 6 unnecessary first rows 
  author <- page %>%
    html_nodes(".author") %>%
    html_text() %>%
    .[-(1:6)] %>%
    trimws(.)
  
  date <- page %>%
    html_nodes(".date") %>%
    html_text() %>%
    .[-(1:6)] %>%
    trimws(.)
  
  this_page_data <- tibble(
    number = number,
    category = category, 
    title = title,
    author = author,
    date = date
  )
  
  hhl_posts <- bind_rows(hhl_posts, this_page_data)
  
  # we scraped the posts for this page
  # now we will scrape the comments for each post on this page
  # there are 30 posts available on each page
  
  # This loop is for comments scraping
  
  for (post in 1:30) {
    
    webElem1 <- remD$findElements("css", ".ab-link")[[post + 6]]
    scroll_to(remD, webElem1)
    webElem1$sendKeysToElement(list(key = "up_arrow"))
    webElem1$sendKeysToElement(list(key = "up_arrow"))
    webElem1$sendKeysToElement(list(key = "up_arrow"))
    webElem1$sendKeysToElement(list(key = "up_arrow"))
    webElem1$highlightElement()
    remD$mouseMoveToLocation(webElement = webElem1)
    zzz(periods = c(1,2))
    webElem1$sendKeysToElement(list(key = "enter"))
    
#    webElem1$clickElement() <- some posts are not clickable so I changed code using sendKeysToElement
  
    # be polite - wait a few seconds per post
    # then scrape the commnets
    # wanted to sleep more time, but posts keep changing so need to fast scraping
    
    zzz(periods = c(3,5))
    print(paste(".....","Crawling for comments of post",post))
    webElem2 <- remD$findElement("css", ".comments-list")
    scroll_to(remD, webElem2)
    post_page <- get_html(remD)
    
    comments <- post_page %>%
    html_nodes(".comment-body") %>%
    html_text() %>%
    trimws(.)
  
    df <- as_tibble(t(tibble(comments = comments)))
    l <- append(l, list(df))
  
    remD$executeScript('window.history.go(-1)')
    }
  
  # Move on to the next page
  webElem <- remD$findElements("css", ".pagination-arrow")[[2]]
  scroll_to(remD, webElem)
  webElem$highlightElement()
  remD$mouseMoveToLocation(webElement = webElem)
  zzz(periods = c(1,1.5))
  webElem$clickElement()
  zzz(periods = c(10,15))
}

# a post with no comment -> also need to be added to the final dataframe
# make a dummy column for those posts to be considered in the rbindlist
new_l <- lapply(l, function (x) if (length(x) == 0) {mutate(x, V1 = NA)} else {x})
combined <- rbindlist(new_l, fill = TRUE)

hhl_posts_comments <- bind_cols(hhl_posts, combined)

```

```{r data engineering}
# converting string to datetime object
# first several data can be expressed as "minutes before" or "hours before"
# this should be converted as the same format with the other data (i.e. "y-m-d")
hhl_posts_comments_clean <- hhl_posts_comments %>%
  mutate(date = ymd(date, tz="Asia/Seoul"))

hhl_posts_comments_clean$date[is.na(hhl_posts_comments_clean$date)] <- hhl_posts_comments_clean$date[length(hhl_posts_comments_clean$date[is.na(hhl_posts_comments_clean$date)]) + 1] + days(1)
```

#### managing date column (Some codes which can be useful)

hhl_posts$date[grepl("분", hhl_posts$date)] <- strsplit(hhl_posts$date[grepl("분", hhl_posts$date)], split = " ") %>% 
  sapply(., "[", 1) %>% 
  sapply(., paste, "m")

hhl_posts$date[grepl("시간", hhl_posts$date)] <- strsplit(hhl_posts$date[grepl("시간", hhl_posts$date)], split = " ") %>% 
  sapply(., "[", 1) %>% 
  sapply(., paste, "h")
  
hhl_posts$date[grepl("m|h", hhl_posts$date)] <- date(hhl_posts$date[length(minutes_and_hours) + 1]) + days(1)


```{r save and close remote driver}
write.csv(hhl_posts,"hhl_posts_UTF.csv", fileEncoding = "UTF-8", row.names = FALSE)

hhl_posts <- read_csv("hhl_posts_UTF.csv")

remD$close()

```

  